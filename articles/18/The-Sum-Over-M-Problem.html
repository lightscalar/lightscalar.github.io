<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>A Discrete Approach to the Sum Over One Problem</title>
  <meta name="description" content="">

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not-->
  
  <link href='//fonts.googleapis.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'>
  

  <!-- Load up MathJax script if needed ... specify in /_data/options.yml file-->
  
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  

  <link rel="stylesheet" type="text/css" href="/css/tufte.css">
  <!-- <link rel="stylesheet" type="text/css" href="/css/print.css" media="print"> -->
  <link rel="shortcut icon" href="/assets/img/ai-logo@2x.png" type="image/png"/>

  <link rel="canonical" href="http://localhost:4000/articles/18/The-Sum-Over-M-Problem">

  <link rel="alternate" type="application/rss+xml" title="Machine Learned" href="http://localhost:4000/feed.xml" />

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-98100846-1', 'auto');
  ga('send', 'pageview');

</script>


</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
    <nav class="group">
      <a href="/">
	<img
	class="badge"
	src="/assets/img/ai-logo@2x.png"
	alt="CH">
      </a>
      
	
      
	
	  
	    <a href="/">Articles</a>
	  
	
      
	
	  
	    <a href="/page/"></a>
	  
	
      
	
	  
	    <a href="/about/">About</a>
	  
	
      
	
	  
	    <a href="/css/print.css"></a>
	  
	
      
	
      
    </nav>
</header>

    <article class="group">
      <h1>A Discrete Approach to the Sum Over One Problem</h1>
<p class="subtitle">January 9, 2018</p>
<hr class="slender">

<!--more-->

<h2 id="black-magic">Black Magic</h2>

<p>Here is an astonishing true thing:</p>

<blockquote>
  <p>The average number of samples that must be drawn from a uniform distribution
on <span>​<script type="math/tex">[0,1]</script></span> before their sum exceeds one is equal to
Euler’s<label for="napier" class="margin-toggle sidenote-number"></label><input type="checkbox" id="napier" class="margin-toggle" /><span class="sidenote">Euler’s number, also know as Napier’s constant,
was actually discovered by the Swiss mathematician Jacob Bernoulli during his
studies of compound interest. It’s value is approximately 2.718281828. It is
the basis of natural logarithms and curiously ubiquitous in mathematics. </span>
number, <span>​<script type="math/tex">e</script></span>.</p>
</blockquote>

<p>I first learned of this fact via a
<a href="https://twitter.com/fermatslibrary/status/924263998589145090">Tweet</a> from
<a href="https://twitter.com/fermatslibrary">@fermatslibrary</a>, and it was a genuine
shock. How in the world does Euler’s number emerge, for no apparent reason, in
response to this innocuous question of probability? Can this even be true? And
if it is, as I. I. Rabi famously said<label for="rabi" class="margin-toggle sidenote-number"></label><input type="checkbox" id="rabi" class="margin-toggle" /><span class="sidenote">Rabi had other good,
quotes, too: “My mother made me a scientist without ever intending to. Every
other Jewish mother in Brooklyn would ask her child after school: So? Did you
learn anything today? But not my mother. “Izzy,” she would say, “did you ask a
good question today?” That difference — asking good questions — made me become
a scientist.” </span> of the discovery of the muon, “Who ordered that?!”.</p>

<p>And yet it is true, provably true, and somewhat well-known. Attached to that
Tweet is an elegant (if terse) proof. Neal Grantham, who terms this the <em>sum
over one</em> problem, takes a <a href="http://nsgrantham.com/sum-over-one">rather more direct
approach</a>, but arrives at the same
delightful conclusion.</p>

<p>These proofs do not fail to convince, but I find them somehow viscerally
unsatisfying.  They offer little insight into why <span>​<script type="math/tex">e</script></span> suddenly
appears, in terms of anything else we know about <span>​<script type="math/tex">e</script></span>. Of course
<span>​<script type="math/tex">e</script></span> assumes many guises, but it is perhaps most famously understood as
the following limit<label for="maor" class="margin-toggle sidenote-number"></label><input type="checkbox" id="maor" class="margin-toggle" /><span class="sidenote">For an excellent overview of <em>e</em>, I
recommend <a href="https://www.amazon.com/Story-Number-Eli-Maor/dp/0691058547">e: The Story of a
Number</a>, by Eli
Maor. The number has a rich and fascinating history, which touches the lives of
many great mathematicians. </span>:</p>

<div class="mathblock"><script type="math/tex; mode=display">\lim_{n\rightarrow\infty}\left(1+\frac{1}{n}\right)^{n} \equiv
e \tag{1}\label{edef}</script></div>

<p>The proofs above fail to inform me who ordered that.</p>

<p>So how does the <span>​<script type="math/tex">e</script></span> of Eq. <span>​<script type="math/tex">\ref{edef}</script></span> make contact with our
question of probability? I wondered if exploring a discrete version of the
problem might demystify <span>​<script type="math/tex">e</script></span>’s bolt-from-the-blue arrival. This article
chronicles these explorations and provides a more organic<label for="me" class="margin-toggle sidenote-number"></label><input type="checkbox" id="me" class="margin-toggle" /><span class="sidenote">According to me. </span> explanation for understanding of why <span>​<script type="math/tex">e</script></span> is the
obvious—or at least plausible—answer.</p>

<h2 id="alea-iacta-est">Alea Iacta Est</h2>

<p>Here is the discrete version of the game. We roll an <span>​<script type="math/tex">n</script></span>-sided die
until the cumulative sum of the throws exceeds <span>​<script type="math/tex">n</script></span>. <label for="dice" class="margin-toggle">⊕</label><input type="checkbox" id="dice" class="margin-toggle" /><span class="marginnote"><img class="fullwidth" src="/assets/img/cubic-dice.png" /><br />Dice. These dice have six sides each, but
we’ll be considering here the more general problem of <em>n</em>-sided dice.</span> We
ask, on average, how many times must we throw that die? This problem is at once
easier to conceptualize than its continuous counterpart, because it is
discrete, and much more difficult to actually calculate—because it is discrete.</p>

<p>Let <span>​<script type="math/tex">K</script></span> be the random variable corresponding to the number of times we
must throw the die until the sum of the throws first exceeds <span>​<script type="math/tex">n</script></span>, the
number of sides on the die. Ultimately we want to find the probability
distribution <span>​<script type="math/tex">P(K=k \mid n)</script></span> — the probability that the sum of our throws
first exceeds <span>​<script type="math/tex">n</script></span> after exactly <span>​<script type="math/tex">k</script></span> throws — because once we
know this, computing the expected value of <span>​<script type="math/tex">k</script></span> is straightforward via
the usual <a href="https://en.wikipedia.org/wiki/Expected_value">definition</a>:</p>

<div class="mathblock"><script type="math/tex; mode=display">\langle k \rangle_n = \sum_{k=2}^{n+1} k
P(K=k \mid n)\tag{2}\label{expect}</script></div>

<p>Note the limits of summation: we’re running from <span>​<script type="math/tex">2</script></span> to
<span>​<script type="math/tex">n+1</script></span>. There is zero chance of exceeding <span>​<script type="math/tex">n</script></span> on the first
roll; the best we can do there is <span>​<script type="math/tex">n</script></span> itself. On the other hand, the
sum is guaranteed to exceed <span>​<script type="math/tex">n</script></span> after <span>​<script type="math/tex">k = n+1</script></span> rolls,
because each roll generates, at minimum, a value of one<label for="first-time" class="margin-toggle sidenote-number"></label><input type="checkbox" id="first-time" class="margin-toggle" /><span class="sidenote">Note that although the probability of this sum exceeding <em>n</em> on
roll <em>n+1</em> is unity, that’s not what we’re calculating. We’re calculating the
probability that the sum <em>first</em> exceeds <em>n</em> on that roll, and not before,
which will be less than one. </span>.</p>

<p>Let <span>​<script type="math/tex">R_i</script></span> represent the random variable corresponding to the
<span>​<script type="math/tex">i^{\text{th}}</script></span> roll. The key to calculating <span>​<script type="math/tex">P(K=k \mid n)</script></span>
is to realize that it must equal the difference between the following
probabilities:</p>

<div class="mathblock"><script type="math/tex; mode=display">P(K=k\mid n) = P(K\leq k\mid n) - P(K\leq k-1\mid n)
\tag{3}\label{key}</script></div>

<p>But note that,</p>

<div class="mathblock"><script type="math/tex; mode=display">P(K \leq k \mid n) = P\left(\sum_{i=1}^{k} R_i > n \right) = 1 - P \left(
\sum_{i=1}^{k} R_i \leq n \right)\tag{4} \label{complement} </script></div>

<p>Using Eq. <span>​<script type="math/tex">\ref{key}</script></span> and Eq. <span>​<script type="math/tex">\ref{complement}</script></span>, we can
write <span>​<script type="math/tex">P(K=k \mid n)</script></span> as,</p>

<div class="mathblock"><script type="math/tex; mode=display"> \begin{align} P(K=k \mid n) &= [1 - P(K \leq k \mid n)] - [1 - P(K
\leq k-1 \mid n)] \tag{5}\\ &= P\left(\sum_{i=1}^{k-1} R_i \leq n\right) -
P\left(\sum_{i=1}^{k} R_i\leq n\right)
\tag{6}\label{prob_diff}\end{align}</script></div>

<p>And now we have only to calculate the probability that the sum of <span>​<script type="math/tex">k</script></span>
throws is less than or equal to <span>​<script type="math/tex">n</script></span>.</p>

<h2 id="counting-is-hard">Counting Is Hard</h2>

<p>We assume that the <span>​<script type="math/tex">k</script></span> throws are independent events, so the
<span>​<script type="math/tex">n^k</script></span> possible outcomes are equally likely. But to compute the
probability, we must know, of these outcomes, how many satisfy <span>​<script type="math/tex">\sum_i R_i
\leq n</script></span>? If we call this number <span>​<script type="math/tex">N_{\leq}(n,k)</script></span>, then the
probability is,</p>

<div class="mathblock"><script type="math/tex; mode=display">P\left(\sum_i R_i \leq n \right) = \frac{N_{\leq}(n,
k)}{n^k} \tag{7}\label{prob_div}</script></div>

<p>So the problem, then, is to compute <span>​<script type="math/tex">N_{\leq}(n,k)</script></span> for arbitrary
<span>​<script type="math/tex">n</script></span> and <span>​<script type="math/tex">k</script></span>.</p>

<p>To find <span>​<script type="math/tex">N_{\leq}</script></span>, let’s first answer a slightly easier question: how
many outcomes will sum to exactly <span>​<script type="math/tex">n</script></span> using <span>​<script type="math/tex">k</script></span> integers?</p>

<p>One way to attack this problem is to write out <span>​<script type="math/tex">n</script></span> as the sum of
<span>​<script type="math/tex">n</script></span> ones (which sum is guaranteed, of course, to sum to <span>​<script type="math/tex">n</script></span>),
and then consider how many ways we can partition those ones into different
groups. This is illustrated in the figure to the right, for the case of
<span>​<script type="math/tex">n=6</script></span> and <span>​<script type="math/tex">k=2</script></span><label for="partitions" class="margin-toggle">⊕</label><input type="checkbox" id="partitions" class="margin-toggle" /><span class="marginnote"><img class="fullwidth" src="/assets/img/partitions@2x.png" /><br />The number of ways we can sum to six via two
positive integers is the number of ways we can select one plus sign from among
the five plus signs in the equation 6 = 1 + 1 + 1 + 1 + 1 + 1, if we regard the
selected plus sign as partitioning the sum into two pieces. The sum to the left
of the selected plus sign represents one number, and the sum to the right of
that plus sign represents the other. If we wanted to sum to exactly six using
three numbers, we’d find the number of ways to select two plus signs from the
five available; and so on.</span>. It is easy to see that the partitioning is
equivalent to asking, “How many ways can I select one plus sign from the five
plus signs?” On the other hand, if we were to ask, how many ways are there to
sum to six using three positive integers, we’d want to calculate how
many ways we can select two plus signs from those five plus signs, and so
forth. This question is answered, in general, using combinations.</p>

<p>Recall that a <a href="https://en.wikipedia.org/wiki/Combination">combination</a> is a
selection of items from a collection such that the order of the selection does
not matter. The number of ways in which one may select <span>​<script type="math/tex">k</script></span> items from
among <span>​<script type="math/tex">n</script></span> available items is given by,</p>

<div class="mathblock"><script type="math/tex; mode=display">\binom{n}{k} = \frac{n!}{k!(n - k)!}</script></div>

<p>By this argument we see that, in general, the number of ways we can sum to
exactly <span>​<script type="math/tex">n</script></span> using <span>​<script type="math/tex">k</script></span> positive integers, which we’ll denote
as <span>​<script type="math/tex">N_{=}(n,k)</script></span>, is given by,</p>

<div class="mathblock"><script type="math/tex; mode=display">N_{=}(n,k)=\binom{n-1}{k-1}=\frac{(n-1)!}{(k-1)! (n - k +
1)!},</script></div>

<p>Using a six-sided die, for example, the number of ways we can sum to six with
two throws is given by,</p>

<div class="mathblock"><script type="math/tex; mode=display">\binom{6-1}{2-1} = \binom{5}{1} = \frac{5!}{1!4!} = 5</script></div>

<p>Note that the five corresponds to the number of possibilities enumerated in the
example to the right. This all seems to hang together.</p>

<p>But Eq. <span>​<script type="math/tex">\ref{prob_div}</script></span> requires the total number of outcomes whose
sum is <em>less than or equal</em> to <span>​<script type="math/tex">n</script></span>. That number will
equal the sum the the number of ways we can reach <span>​<script type="math/tex">n^\prime</script></span> for all
<span>​<script type="math/tex">k \leq n^\prime \leq n</script></span>:</p>

<div class="mathblock"><script type="math/tex; mode=display">N_{\leq}(n,k) = \sum_{n^\prime=k}^{n} N_=(n^\prime,
k) = \sum_{n^\prime=k}^{n} \binom{n^\prime-1}{k-1}=\binom{n}{k}</script></div>

<p>where in the last step we’ve used the so-called <a href="https://en.wikipedia.org/wiki/Hockey-stick_identity">hockey-stick
identity</a> of combinations.</p>

<p>This is a delightfully simple result<label for="ubiquity" class="margin-toggle sidenote-number"></label><input type="checkbox" id="ubiquity" class="margin-toggle" /><span class="sidenote">Delightful, but
hardly surprising. Most interesting counting problems wend their way back to
<a href="https://en.wikipedia.org/wiki/Pascal%27s_triangle">Pascal’s triangle</a>
eventually, it seems. </span> that allows us to compute the probability of interest:</p>

<div class="mathblock"><script type="math/tex; mode=display">P\left(\sum_{i=1}^{k} R_i \leq n\right) = \frac{1}{n^k} \binom{n}{k}
\tag{8} \label{less-than} </script></div>

<p>And this, in turn, allows us to calculate the probability in Eq.
<span>​<script type="math/tex">\ref{prob_div}</script></span>, <span>​<script type="math/tex">P(K=k \mid n)</script></span>. Using Eq.
<span>​<script type="math/tex">\ref{prob_diff}</script></span> and <span>​<script type="math/tex">\ref{less-than}</script></span>,</p>

<div class="mathblock"><script type="math/tex; mode=display">P(K=k) = \frac{1}{n^{k-1}} \binom{n}{k-1} - \frac{1}{n^k}
\binom{n}{k} \tag{9} \label{k-is-k}</script></div>

<p>And now finally, we’re in a position to compute <span>​<script type="math/tex">\langle k \rangle</script></span>
via Eq. <span>​<script type="math/tex">\ref{expect}</script></span>.</p>

<p>But, well, after all these perambulations, we might wonder if we’ve made a
mistake. In these cases, it is prudent to turn to numerical simulation to
verify our analytic results. Let’s check our sanity by verifying Eq.
<span>​<script type="math/tex">\ref{k-is-k}</script></span> by way of simulation. We do this by simulating our die
throwing experiment in Python using a synthetic die with <span>​<script type="math/tex">n=10</script></span> sides,
and compare the empirically measured probabilities with our theoretically
calculated results, via Eq. <span>​<script type="math/tex">\ref{k-is-k}</script></span>. The results of this, for
<span>​<script type="math/tex">10^6</script></span> simulated rounds, is presented in the figure below:</p>

<figure><figcaption>The probability that the sum of
successive throws of a ten-sided die exceeds ten, after k throws. Note that
the empirical measurements (blue bars) match theoretical predictions (red dots)
quite well.</figcaption><img src="/assets/img/distributions.png" /></figure>

<p>Now that we’ve convinced ourselves that we are correctly computing the
probability <span>​<script type="math/tex">P(K=k \mid n)</script></span>, we are <em>finally</em> in a position to
calculate the expectation we’re after.</p>

<h2 id="great-expectations">Great Expectations</h2>

<p>Before computing <span>​<script type="math/tex">\langle k \rangle</script></span>, let’s rearrange Eq.
<span>​<script type="math/tex">\ref{k-is-k}</script></span> to make it a bit simpler. Note that with a little
algebra we can write,</p>

<div class="mathblock"><script type="math/tex; mode=display">\binom{n}{k-1} = \frac{n!}{(k-1)!(n-k+1)!} =
\frac{k}{(n-k+1)}\frac{n!}{k!(n-k)!}=\frac{k}{(n-k+1)}\binom{n}{k}</script></div>

<p>This allows us to write Eq. <span>​<script type="math/tex">\ref{k-is-k}</script></span>, with yet a bit more
algebra, as,</p>

<div class="mathblock"><script type="math/tex; mode=display">P(K=k \mid n) = \frac{n(k-1)+(k-1)}{n^k(n-k+1)} \binom{n}{k} </script></div>

<p>Multiplying by <span>​<script type="math/tex">k</script></span> and summing, as per Eq. <span>​<script type="math/tex">\ref{expect}</script></span>, we
obtain,</p>

<div class="mathblock"><script type="math/tex; mode=display"> \langle k \rangle_n = \sum_{k=2}^{n+1}\frac{nk(k-1) + k(k-1)}
{n^k(n-k+1)} \binom{n}{k} \tag{10}\label{answer}</script></div>

<p>And there we are. For a die with <span>​<script type="math/tex">n</script></span> sides we can now compute the
expected number of throws we’ll need to exceed <span>​<script type="math/tex">n</script></span>. What do we expect
for a six-sided die? For <span>​<script type="math/tex">n=6</script></span>, Eq. <span>​<script type="math/tex">\ref{answer}</script></span> yields,</p>

<div class="mathblock"><script type="math/tex; mode=display"> \langle k \rangle_{n=6} = \frac{117649}{46656} \approx 2.522
</script></div>

<p>a number that is decidedly not <span>​<script type="math/tex">e</script></span><label for="ofcourse" class="margin-toggle sidenote-number"></label><input type="checkbox" id="ofcourse" class="margin-toggle" /><span class="sidenote">Not that we’d
expect it to be, of course; rational numbers tend never to be irrational. </span>.
But, interestingly, neither is it too far off. Let’s consider a die with
more sides, say, <span>​<script type="math/tex">n=15</script></span>:</p>

<div class="mathblock"><script type="math/tex; mode=display"> \langle k \rangle_{n=15} =
\frac{11152921504606846976}{437893890380859375} \approx 2.633 </script></div>

<p>Even closer! Intrigued by these observations, let’s keep increasing the number
of sides on our die. If we continue calculating the expectation for ever higher
values of <span>​<script type="math/tex">n</script></span>, we generate the following plot:</p>

<figure><figcaption>The mean number of die throws
required before sum of throws exceeds the number of sides on the die, as a
function of the number of sides on the die. Note that this mean value seems
to be converging.</figcaption><img src="/assets/img/convergence.png" /></figure>

<p>It appears that, as we let <span>​<script type="math/tex">n</script></span> increase, the expected number of throws
required for the sum to exceed <span>​<script type="math/tex">n</script></span> converges to a number — and that
number, empirically at least, appears to be <span>​<script type="math/tex">e</script></span>. But can we prove it?
Sure we can.</p>

<p>Let’s write out Eq. <span>​<script type="math/tex">\ref{answer}</script></span> in all its glory,</p>

<div class="mathblock"><script type="math/tex; mode=display">\langle k \rangle_n = \sum_{k=2}^{n+1} \frac{n k(k-1) + k(k-1)}{n^k
(n-k+1)} \frac{n!}{k!(n-k)!}</script></div>

<p>Now consider the case where <span>​<script type="math/tex">n\gg k</script></span>. The above expression simplifies,</p>

<div class="mathblock"><script type="math/tex; mode=display"> \begin{align} \langle k \rangle_n &\approx \sum_{k=2}^{n+1}
\frac{k(k-1)}{n^k}\frac{n!}{k!(n-k)!} \\ &= \sum_{k=2}^{n+1}\frac{n!}{n^k
(k-2)! (n-k)!} \\ &= \sum_{k=2}^{n+1}\frac{n! (n-k+2) (n-k+1)}{(k-2)!(n - k +
2)!} \frac{1}{n^k} \\ &\approx \sum_{k=2}^{n} \frac{n!}{(k-2)![n - (k-2)]!}
\frac{1}{n^{k-2}} \end{align} </script></div>

<p>And now note that we can change the lower limit of the summation; rather than
starting at <span>​<script type="math/tex">k=2</script></span>, we can start at <span>​<script type="math/tex">k=0</script></span>, provided we replace
all the <span>​<script type="math/tex">(k-2)</script></span> terms that appear in our summand with <span>​<script type="math/tex">k</script></span>.
Doing this we obtain,</p>

<div class="mathblock"><script type="math/tex; mode=display">\langle k \rangle_n \approx \sum_{k=0}^{n} \frac{1}{n^k}
\frac{n!}{k!(n-k)!} = \sum_{k=0}^{n} \frac{1}{n^k} \binom{n}{k} </script></div>

<p>where, again, this holds for <span>​<script type="math/tex">n \gg k</script></span>. This may begin to look
familiar. In particular, recall the <a href="https://en.wikipedia.org/wiki/Binomial_theorem">binomial
theorem</a>, which states that,</p>

<div class="mathblock"><script type="math/tex; mode=display">(1+x)^n = \sum_{k=0}^n x^k \binom{n}{k} </script></div>

<p>In our present situation, letting <span>​<script type="math/tex">x = 1/n</script></span>, we can write our
expectation as,</p>

<div class="mathblock"><script type="math/tex; mode=display">\lim_{n \rightarrow \infty} \langle k \rangle_n = \lim_{n \rightarrow
\infty} \sum_{k=0}^{n} \frac{1}{n^k} \binom{n}{k} = \lim_{n \rightarrow \infty}
\left(1+ \frac{1}{n}\right)^n \equiv e</script></div>

<p>And we’ve recovered the definition of Euler’s number, <em>à la</em> Eq.
<span>​<script type="math/tex">\ref{edef}</script></span>!</p>

<p>Here is where I find <span>​<script type="math/tex">e</script></span> emerge organically, in a sense. For all
finite values of <span>​<script type="math/tex">n</script></span>, the expectation is a rational number. But as the
number of sides increases, the combinatorics of how the throws can sum to
exceed <span>​<script type="math/tex">n</script></span> evolves in just the right way such that—as <span>​<script type="math/tex">n</script></span>
gets really large—it starts to converge to the binomial expansion of
<span>​<script type="math/tex">(1+1/n)^n</script></span>, which in the limit turns out to be exactly the definition
of <span>​<script type="math/tex">e</script></span>. And for my part, at least, I can start to see a glimmer of an
answer to that primal question: <em>who ordered that?</em><label for="ofcourse" class="margin-toggle sidenote-number"></label><input type="checkbox" id="ofcourse" class="margin-toggle" /><span class="sidenote">Of
course, you might argue, the other proofs I’ve cited could be brought to this
same point—the binomial expansion could ultimately be reconstructed from those
calculations as well. Certainly, I’d agree, they must all be equivalent at some
level, but seeing the binomial theorem emerge directly from the combinatorics
of die throws and lead to the very definition of Euler’s constant makes me feel
happy inside. </span></p>

<h2 id="from-discrete-back-to-continuous">From Discrete Back to Continuous</h2>

<p>As <span>​<script type="math/tex">n</script></span> heads off to infinity, our discrete problem becomes equivalent
to the original, continuous problem introduced at the start. To better see the
equivalence, note we can map the discrete problem onto the unit interval, as in
the following illustration.</p>

<figure><figcaption>We can reconstruct our discrete
die-throwing experiment on the unit interval by partitioning it into
subintervals. As the number of subintervals approaches infinity—equivalent to
our infinite sided die—we recover the uniform distribution we introduced
about originally.</figcaption><img src="/assets/img/real-line@2x.png" /></figure>

<p>Rather than considering an <span>​<script type="math/tex">n</script></span>-sided die, we partition the unit
interval into <span>​<script type="math/tex">n</script></span> subintervals. At each step, we then sample a number
uniformly from the interval <span>​<script type="math/tex">[0,1]</script></span>, and select as our value the
uppermost limit of whatever subinterval the sample falls into. We continue to
select values in this manner until their sum exceeds one, just as in the
original formulation of the problem—but now we’re working in the context of a
discrete problem. And now, as <span>​<script type="math/tex">n \rightarrow \infty</script></span>, discrete meets
continuous, and by the arguments advanced above, we’ve proven the original
assertion.</p>




    </article>
    <span class="print-footer">A Discrete Approach to the Sum Over One Problem - January 9, 2018 - Matthew J. Lewis</span>
    <footer>
  <hr class="slender">
  <ul class="footer-links">
    <li><a href="mailto:mlewis@michaero.com"><span class="icon-mail"></span></a></li>    
    
      <li>
        <a href="//www.twitter.com/lightscalar"><span class="icon-twitter"></span></a>
      </li>
    
      <li>
        <a href="//github.com/lightscalar"><span class="icon-github"></span></a>
      </li>
      
  </ul>
<div class="credits">
<span>&copy; 2018 &nbsp;&nbsp;MATTHEW J. LEWIS</span></br> <br>
</div>  
</footer>

  </body>
</html>
